{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing steps include:\n",
    "1. Select those pubmed records for which citation information is available\n",
    "2. Tokenize abstracts and keywords using NLTK\n",
    "3. Detect acronyms and replace them with full form\n",
    "4. Trigram transformation\n",
    "5. Part of speech (POS) tagging, lemmatization, stop-words removal using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Bio import Medline\n",
    "import nltk.data\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from IPython.core.pylabtools import figsize\n",
    "from matplotlib import pyplot as plt\n",
    "figsize(11, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 945 paper records.\n"
     ]
    }
   ],
   "source": [
    "fin = open('SWI_QSM_papers.txt','r')\n",
    "records = Medline.parse(fin)\n",
    "papers=list(records)\n",
    "print('Found {0} paper records.'.format(len(papers)))\n",
    "fin.close()\n",
    "abstract = papers[0]['AB']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract the citation information\n",
    "Create query keys using the first 7 words in title + year of publication. Note that sometimes, for the same paper, the year may appear to be different in Pubmed and google scholar search results. When extracting the citation information, we need to try the keys with year+/-1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "citation_data=pd.read_pickle('CitationData.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer using regular expression for citation extraction\n",
    "tokenizer_word_reg = RegexpTokenizer(\"[\\w]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#citation data\n",
    "citation=dict()\n",
    "year = dict()\n",
    "for i in range(citation_data.shape[0]):\n",
    "    temp = tokenizer_word_reg.tokenize(citation_data.iloc[i,:]['title'].lower().replace('-',' '))    \n",
    "    key = '_'.join(temp[0:7]) if len(temp)>=7 else '_'.join(temp)\n",
    "    key = key + '_' + str(citation_data.iloc[i,:]['year'])\n",
    "    citation[key]=citation_data.iloc[i,:]['num_citations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize the title, keywords and abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def acronym_detection(text, delimiter='_', option=1):\n",
    "    # function for detection potential acronyms. \n",
    "    # output: dictionary containing {acronym: full-form}\n",
    "    # text are assumed to be tokenized already: [tokens]\n",
    "    if option==2: #keywords\n",
    "        text_temp = []\n",
    "        for word in text:\n",
    "            if ',' in word or '(' in word:\n",
    "                word = nltk.word_tokenize(word)\n",
    "                for w in word:\n",
    "                    text_temp.append(w)\n",
    "            else:\n",
    "                text_temp.append(word)\n",
    "        text = text_temp\n",
    "    N= len(text) \n",
    "    terms=dict()\n",
    "    for ind, word in enumerate(text):\n",
    "        if word.isupper(): \n",
    "            m = len(word)\n",
    "            if ind>=m+1:\n",
    "                words_before = text[(ind- m -1):(ind-1)]\n",
    "                a1 = [w[0].upper() for w in words_before]\n",
    "                a1 = ''.join(a1)\n",
    "                if a1==word:\n",
    "                    terms[a1]=delimiter.join([w.lower() for w in words_before])\n",
    "            if ind<=N-2-m:\n",
    "                words_after = text[(ind+2):(ind+2+m)]\n",
    "                a2 = [w[0].upper() for w in words_after]\n",
    "                a2 = ''.join(a2)\n",
    "                if a2==word:\n",
    "                    terms[a2]=delimiter.join([w.lower() for w in words_after])\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords may contain acronyms. The acronyms will be detected and subsequently removed, with the full form joined by '_'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def keyword_tokenizer(keywords, deliminator='_'):\n",
    "    temp = keywords\n",
    "    for j, kw in enumerate(temp):\n",
    "        if ',' in kw:\n",
    "            kw1=kw.split(',')\n",
    "            for kw11 in kw1:\n",
    "                if kw11.islower():\n",
    "                    kw = kw11.strip().replace(' ', deliminator) if ' ' in kw11 else kw11\n",
    "                    break\n",
    "        kw = re.sub(r'\\([^)]*\\)', '', kw).strip().replace('-',' ').replace(' ', deliminator) #remove contents in parentheses\n",
    "        temp[j] = kw.lower()\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N=len(papers)\n",
    "texts = [] #abstract tokens\n",
    "tokens_t = [] #titles token\n",
    "tokens_k = [] #keywords token\n",
    "num_citation = [] \n",
    "year = []\n",
    "exclusion_list = [] #papers without keywords or abstracts or number of citations\n",
    "acronyms = dict()\n",
    "authors = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished tokenizing 265 papers. There are 50 papers without abstracts, 630 papers without citation info.\n",
      "Detected 13 acronyms.\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(0, N):\n",
    "    if 'AB' not in papers[i].keys():\n",
    "        exclusion_list.append(i) #abstract is missing\n",
    "        continue;\n",
    "    ### 1. tokenize the titles. \n",
    "    temp_t = tokenizer_word_reg.tokenize(papers[i]['TI'].lower().replace('-',' ')) #initial tokens of the title\n",
    "    \n",
    "    ### 2. check the number of citations and year of publication\n",
    "    key = '_'.join(temp_t[0:7]) if len(temp_t)>=7 else '_'.join(temp_t)\n",
    "    \n",
    "    for yy in [-1,0,1]:\n",
    "        query_key = key + '_' + str(int(papers[i]['DA'][:4])+yy)\n",
    "        temp_citation = citation.get(query_key,-1)\n",
    "        if temp_citation != -1:\n",
    "            break\n",
    "    \n",
    "    if temp_citation==-1:\n",
    "        exclusion_list.append(i) #citation not available\n",
    "        count+=1\n",
    "        continue;\n",
    "        \n",
    "    tokens_t.append(temp_t)\n",
    "    authors.append(papers[i]['AU'])\n",
    "    num_citation.append(temp_citation)\n",
    "    year.append(papers[i]['DA'][:4])\n",
    "            \n",
    "    ### 3. tokenize the abstract\n",
    "    abstract = papers[i]['AB'].replace('-',' ') # remove the hiphen, for better detection of N-gram.\n",
    "    abs_token= nltk.word_tokenize(abstract.lower()) # tokenize the abstract\n",
    "    \n",
    "    if 'OT' in papers[i].keys():\n",
    "        ### 4. check acronyms in keywords\n",
    "        acronyms_temp = acronym_detection(papers[i]['OT'], delimiter='_', option=2)\n",
    "        if len(acronyms_temp)>0:\n",
    "            acronyms.update(acronyms_temp)\n",
    "        ### tokenize keywords\n",
    "        tokens_k.append(keyword_tokenizer(papers[i]['OT']))\n",
    "    else:\n",
    "        tokens_k.append([])\n",
    "\n",
    "    ### 5. check acronyms in abstract\n",
    "    acronyms_temp = acronym_detection(abs_token, delimiter='_', option=1)\n",
    "    if len(acronyms_temp)>0:\n",
    "        acronyms.update(acronyms_temp)\n",
    "    \n",
    "    ### 6. tokenize the abstract after removing contents in parentheses\n",
    "    abstract = re.sub(r'\\([^)]*\\)', '', abstract)\n",
    "    abs_token= nltk.word_tokenize(abstract.lower()) # tokenize the abstract\n",
    "    texts.append(abs_token)        \n",
    "print('Finished tokenizing {} papers. There are {} papers without abstracts, {} papers without citation info.'\n",
    "      .format(N-len(exclusion_list), len(exclusion_list)-count, count))\n",
    "print('Detected {} acronyms.'.format(len(acronyms)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with acronyms\n",
    "Acronyms or abbreviations are usually used in research articles. We extract the acronyms which are defined in keywords or abstracts, and replace the acronyms with the full-form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CC': 'ccg_ccs',\n",
       " 'DGM': 'deep_grey_matter',\n",
       " 'DGMPM': 'deep_gray_matter_parcellation_map',\n",
       " 'DTI': 'diffusion_tensor_imaging',\n",
       " 'ICA': 'independent_component_analysis',\n",
       " 'MRI': 'magnetic_resonance_imaging',\n",
       " 'MSA': 'magnetic_susceptibility_anisotropy',\n",
       " 'PDE': 'partial_differential_equation',\n",
       " 'QSM': 'quantitative_susceptibility_mapping',\n",
       " 'SS': 'segmentation_sophisticated harmonic artifact reduction for phase data',\n",
       " 'STI': 'susceptibility_tensor_imaging',\n",
       " 'SWI': 'susceptibility_weighted_imaging',\n",
       " 'TBI': 'traumatic_brain_injury'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### 7. replace acronyms with full form. \n",
    "abbreviation = acronyms.keys()\n",
    "texts1=[]\n",
    "for sublist in texts:\n",
    "    temp = []\n",
    "    for item in sublist:\n",
    "        if item.upper() in abbreviation:\n",
    "            temp.append(acronyms[item.upper()])\n",
    "        else:\n",
    "            temp.append(item)\n",
    "    texts1.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trigram transform\n",
    "bigram = gensim.models.Phrases(texts1, threshold = 20)\n",
    "trigram= gensim.models.Phrases(bigram[texts1], threshold = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['susceptibility_weighted_imaging', 'is', 'a', 'magnetic_resonance_imaging', 'technique', 'that', 'enhances', 'image', 'contrast', 'by', 'using', 'the', 'susceptibility', 'differences_between', 'tissues', '.', 'it', 'is', 'created', 'by', 'combining', 'both', 'magnitude', 'and', 'phase', 'in', 'the', 'gradient_echo', 'data', '.', 'susceptibility_weighted_imaging', 'is', 'sensitive', 'to', 'both', 'paramagnetic', 'and', 'diamagnetic', 'substances', 'which', 'generate', 'different', 'phase_shift', 'in', 'magnetic_resonance_imaging', 'data', '.', 'susceptibility_weighted_imaging', 'images', 'can_be', 'displayed', 'as', 'a', 'minimum', 'intensity', 'projection', 'that', 'provides', 'high_resolution', 'delineation', 'of', 'the', 'cerebral', 'venous', 'architecture', ',', 'a', 'feature', 'that', 'is', 'not', 'available', 'in', 'other', 'magnetic_resonance_imaging', 'techniques', '.', 'as', 'such', ',', 'susceptibility_weighted_imaging', 'has_been', 'widely', 'applied', 'to', 'diagnose', 'various', 'venous', 'abnormalities', '.', 'susceptibility_weighted_imaging', 'is', 'especially', 'sensitive', 'to', 'deoxygenated', 'blood', 'and', 'intracranial', 'mineral', 'deposition', 'and', ',', 'for', 'that', 'reason', ',', 'has_been', 'applied', 'to', 'image', 'various', 'pathologies', 'including', 'intracranial', 'hemorrhage', ',', 'traumatic_brain_injury', ',', 'stroke', ',', 'neoplasm', ',', 'and', 'multiple_sclerosis', '.', 'susceptibility_weighted_imaging', ',', 'however', ',', 'does', 'not', 'provide', 'quantitative', 'measures', 'of', 'magnetic_susceptibility', '.', 'this', 'limitation', 'is', 'currently', 'being', 'addressed', 'with', 'the', 'development', 'of', 'quantitative_susceptibility_mapping', 'and', 'susceptibility_tensor_imaging', '.', 'while', 'quantitative_susceptibility_mapping', 'treats', 'susceptibility', 'as', 'isotropic', ',', 'susceptibility_tensor_imaging', 'treats', 'susceptibility', 'as', 'generally', 'anisotropic', 'characterized', 'by', 'a', 'tensor', 'quantity', '.', 'this_article', 'reviews', 'the', 'basic', 'principles', 'of', 'susceptibility_weighted_imaging', ',', 'its', 'clinical', 'and', 'research', 'applications', ',', 'the', 'mechanisms', 'governing', 'brain', 'susceptibility', 'properties', ',', 'and', 'its', 'practical', 'implementation', ',', 'with', 'a', 'focus', 'on', 'brain', 'imaging', '.']\n"
     ]
    }
   ],
   "source": [
    "print(trigram[bigram[texts1[0]]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_option=1 #no JJ\n",
    "texts2=[]\n",
    "english_stops = set(stopwords.words('english'))\n",
    "\n",
    "for i in range(len(texts1)):\n",
    "    # POS tagging\n",
    "    text_tag = nltk.pos_tag(list(trigram[bigram[texts1[i]]]))\n",
    "    text_temp = []\n",
    "    for word, tag in text_tag:\n",
    "        if pos_option==1:\n",
    "            if 'NN' in tag:\n",
    "                text_temp.append(word)\n",
    "        elif pos_option==2:\n",
    "            if 'NN' in tag or 'VB' in tag:\n",
    "                text_temp.append(word)\n",
    "        else:\n",
    "            if 'NN' in tag or 'VB' in tag or 'JJ' in tag:\n",
    "                text_temp.append(word)\n",
    "    # lemmatization, remove stop-words            \n",
    "    lemmatizer = nltk.WordNetLemmatizer()\n",
    "    text_temp = [lemmatizer.lemmatize(word) for word in text_temp]\n",
    "    # remove stop-words, remove numbers\n",
    "    text_temp = [word for word in text_temp if word not in english_stops and not word.isdigit() and word!= '%']\n",
    "    \n",
    "    #combine the abstract tokens and keywords tokens\n",
    "    texts2.append(text_temp + tokens_k[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['susceptibility_weighted_imaging', 'magnetic_resonance_imaging', 'technique', 'image', 'contrast', 'susceptibility', 'tissue', 'magnitude', 'phase', 'gradient_echo', 'data', 'susceptibility_weighted_imaging', 'substance', 'phase_shift', 'data', 'image', 'intensity', 'projection', 'high_resolution', 'delineation', 'architecture', 'feature', 'technique', 'abnormality', 'susceptibility_weighted_imaging', 'blood', 'mineral', 'deposition', 'reason', 'has_been', 'image', 'pathology', 'hemorrhage', 'traumatic_brain_injury', 'stroke', 'neoplasm', 'multiple_sclerosis', 'susceptibility_weighted_imaging', 'measure', 'magnetic_susceptibility', 'limitation', 'development', 'treat', 'susceptibility', 'isotropic', 'treat', 'susceptibility', 'tensor', 'quantity', 'this_article', 'principle', 'susceptibility_weighted_imaging', 'research', 'application', 'mechanism', 'brain', 'susceptibility', 'property', 'implementation', 'focus', 'brain', 'imaging', 'magnetic_resonance_imaging', 'magnetic_susceptibility_anisotropy', 'quantitative_susceptibility_mapping', 'susceptibility_tensor_imaging', 'susceptibility_weighted_imaging', 'traumatic_brain_injury', 'hemorrhage', 'iron', 'multiple_sclerosis', 'myelin', 'stroke']\n"
     ]
    }
   ],
   "source": [
    "print(texts2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pickle.dump((texts2, tokens_t, tokens_k, num_citation, year, acronyms), open(\"data/tokens_ab_key.p\",'wb')) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
